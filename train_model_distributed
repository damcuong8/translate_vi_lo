def train_model_distributed(rank, world_size, config):
    """Train the model in distributed mode.
    Args:
        rank: Current process rank
        world_size: Total number of processes
        config: Training configuration
    """
    # Set up distributed training environment
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # Initialize process group
    dist.init_process_group(
        backend=config.get('backend', 'nccl'),
        rank=rank,
        world_size=world_size
    )
    
    # Get device for current process
    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() else "cpu")
    
    if rank == 0:
        print(f"Training with distributed data parallel on {world_size} GPUs")
    
    # Set device specific seed for reproducibility
    torch.manual_seed(42 + rank)
    
    # Make sure the weights folder exists on the main process
    if rank == 0:
        Path(f"{config['datasource']}_{config['model_folder']}").mkdir(parents=True, exist_ok=True)
    
    # Wait for the main process to create directories
    dist.barrier()
    
    # Get dataloaders, tokenziers, and samplers
    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt, train_sampler = get_ds(
        config, 
        is_distributed=True, 
        rank=rank, 
        world_size=world_size
    )
    
    # Create model and move to correct device
    model = get_model(config, tokenizer_src.vocab_size(), tokenizer_tgt.vocab_size(), device)
    
    # Wrap model in DDP
    model = DDP(
        model, 
        device_ids=[rank] if torch.cuda.is_available() else None,
        output_device=rank if torch.cuda.is_available() else None,
        find_unused_parameters=config.get('find_unused_parameters', False)
    )
    
    # Initialize TensorBoard writer (only on main process)
    writer = None
    if rank == 0:
        writer = SummaryWriter(config['experiment_name'])

    # Use AdamW optimizer with weight decay
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=config['lr'], 
        eps=1e-9,
        weight_decay=config.get('weight_decay', 0.01)
    )

    # If the user specified a model to preload before training, load it
    initial_epoch = 0
    global_step = 0
    preload = config['preload']
    model_filename = None
    
    if preload and rank == 0:
        # Determine the model filename to load
        if preload == 'latest':
            model_filename = latest_weights_file_path(config)
        elif os.path.exists(preload) and os.path.isfile(preload):
            # If preload is a direct path to a file that exists, use it directly
            model_filename = preload
            print(f'Using direct model path: {model_filename}')
        else:
            # Otherwise use the standard pattern
            model_filename = get_weights_file_path(config, preload)
            
        if model_filename and os.path.exists(model_filename):
            print(f'Rank {rank}: Loading model from {model_filename}')
            state = torch.load(model_filename, map_location=device)
            model.module.load_state_dict(state['model_state_dict'])
            initial_epoch = state['epoch'] + 1
            optimizer.load_state_dict(state['optimizer_state_dict'])
            global_step = state.get('global_step', 0)
            print(f'Rank {rank}: Loaded checkpoint. Resuming from epoch {initial_epoch}, global step {global_step}')
        else:
            print(f'Rank {rank}: No model to preload, starting from scratch') 